name: dbt run
# This workflow is triggered on pushes to the repository.
on: [push]

jobs:
  build:
    name: Run dbt, export images and deploy docs site
    runs-on: ubuntu-latest
    env:
      POSTGRES_HOSTNAME: ${{ secrets.POSTGRES_HOSTNAME }}
      POSTGRES_USERNAME: ${{ secrets.POSTGRES_USERNAME }}
      POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
      POSTGRES_PORT: ${{ secrets.POSTGRES_PORT }}
      POSTGRES_DB_NAME: github_dbt
      POSTGRES_SCHEMA_NAME: artwork
      DOCS_S3_BUCKET: omnata-sql-draw-archive-test
    
    steps:
      - name: Checkout current repo
        uses: actions/checkout@v1

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_IAM_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_IAM_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Extract branch name
        shell: bash
        run: echo "##[set-output name=branch;]$(echo ${GITHUB_REF#refs/heads/})"
        id: extract_branch

      - name: 'Run dbt'
        run: |
          export POSTGRES_HOSTNAME=${{ env.POSTGRES_HOSTNAME }}
          export POSTGRES_USERNAME=${{ env.POSTGRES_USERNAME }}
          export POSTGRES_PASSWORD=${{ env.POSTGRES_PASSWORD }}
          chmod +x ./ci/run_dbt.sh
          ./ci/run_dbt.sh
          mkdir target/images

      - name: 'Render images'
        run: |
          export POSTGRES_HOSTNAME=${{ env.POSTGRES_HOSTNAME }}
          export POSTGRES_USERNAME=${{ env.POSTGRES_USERNAME }}
          export POSTGRES_PASSWORD=${{ env.POSTGRES_PASSWORD }}
          pip install -r requirements.txt
          # Queries all the models in the database, uses matplotlib to render pngs from them
          python ./ci/render_images_from_db.py
          # Modifies the docs manifest.json to include images of each model
          python ./ci/add_image_links_to_manifest.py

      - name: 'Copy docs to s3'
        run: |
          # copy the docs folder recursively over to s3
          aws s3 cp target s3://${{ env.DOCS_S3_BUCKET }}/ --recursive --acl public-read

      - name: 'Concatenate all macros into one sql file, and copy to s3'
        run: |
          find macros -name '*.sql' -not -name 'new_layer_materialization.sql' -exec cat {} \; > all_macros.sql
          aws s3 cp all_macros.sql s3://${{ env.DOCS_S3_BUCKET }}/all_macros.sql --acl public-read         

      - name: 'Master builds - do something else'
        if: contains(github.ref, 'main')
        run: echo "hi"




